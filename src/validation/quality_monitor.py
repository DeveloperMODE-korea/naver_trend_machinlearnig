"""
í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ

ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ì¶”ì , ê²½ê³  ì‹œìŠ¤í…œ, í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import json
import os
from ..config import Config
from ..utils.file_utils import FileManager

class QualityMonitor:
    """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, quality_thresholds: Dict[str, float] = None):
        """ì´ˆê¸°í™”
        
        Args:
            quality_thresholds: í’ˆì§ˆ ì„ê³„ê°’ ë”•ì…”ë„ˆë¦¬
        """
        self.quality_thresholds = quality_thresholds or {
            'completeness_min': 85.0,      # ì™„ì„±ë„ ìµœì†Œ 85%
            'validity_min': 90.0,          # ìœ íš¨ì„± ìµœì†Œ 90%
            'consistency_min': 95.0,       # ì¼ê´€ì„± ìµœì†Œ 95%
            'sufficiency_min': 70.0,       # ì¶©ë¶„ì„± ìµœì†Œ 70%
            'outlier_max': 5.0,            # ì´ìƒì¹˜ ìµœëŒ€ 5%
            'prediction_error_max': 30.0,  # ì˜ˆì¸¡ ì˜¤ì°¨ ìµœëŒ€ 30%
        }
        
        self.quality_history = []
        self.alerts = []
        self.monitor_log = []
    
    def assess_data_quality(self, data_df: pd.DataFrame, 
                          validator_results: Dict = None,
                          outlier_results: Dict = None) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ ë°ì´í„° í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤
        
        Args:
            data_df: í‰ê°€í•  ë°ì´í„°í”„ë ˆì„
            validator_results: DataValidatorì˜ ê²°ê³¼
            outlier_results: OutlierDetectorì˜ ê²°ê³¼
            
        Returns:
            í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        assessment_time = datetime.now()
        
        # 1. ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
        quality_metrics = self._calculate_basic_metrics(data_df)
        
        # 2. ì´ìƒì¹˜ ê´€ë ¨ ë©”íŠ¸ë¦­
        if outlier_results:
            quality_metrics.update(self._calculate_outlier_metrics(outlier_results))
        
        # 3. ì˜ˆì¸¡ í’ˆì§ˆ ë©”íŠ¸ë¦­ (ìˆëŠ” ê²½ìš°)
        if validator_results:
            quality_metrics.update(self._calculate_prediction_metrics(validator_results))
        
        # 4. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(quality_metrics)
        
        # 5. í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        quality_grade = self._determine_quality_grade(overall_score)
        
        # 6. ê²½ê³  ìƒì„±
        alerts = self._generate_alerts(quality_metrics)
        
        # 7. ê°œì„  ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_recommendations(quality_metrics)
        
        assessment_result = {
            'timestamp': assessment_time,
            'data_count': len(data_df),
            'keyword_count': len(data_df['keyword'].unique()),
            'quality_metrics': quality_metrics,
            'overall_score': overall_score,
            'quality_grade': quality_grade,
            'alerts': alerts,
            'recommendations': recommendations,
            'passed_thresholds': self._check_thresholds(quality_metrics)
        }
        
        # ì´ë ¥ ì €ì¥
        self.quality_history.append(assessment_result)
        self.alerts.extend(alerts)
        
        self._log_assessment(assessment_result)
        
        return assessment_result
    
    def _calculate_basic_metrics(self, data_df: pd.DataFrame) -> Dict[str, float]:
        """ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
        metrics = {}
        
        # ì™„ì„±ë„ (ê²°ì¸¡ê°’ ë¹„ìœ¨)
        total_cells = len(data_df) * len(data_df.select_dtypes(include=[np.number]).columns)
        missing_cells = data_df.select_dtypes(include=[np.number]).isnull().sum().sum()
        metrics['completeness'] = (1 - missing_cells / total_cells) * 100 if total_cells > 0 else 100
        
        # ìœ íš¨ì„± (ë²”ìœ„ ë‚´ ê°’ ë¹„ìœ¨)
        valid_ratio_count = ((data_df['ratio'] >= 0) & (data_df['ratio'] <= 100)).sum()
        metrics['validity'] = (valid_ratio_count / len(data_df)) * 100
        
        # ì¼ê´€ì„± (ë°ì´í„° íƒ€ì… ì¼ê´€ì„±)
        consistency_issues = 0
        if not pd.api.types.is_datetime64_any_dtype(data_df['date']):
            consistency_issues += 1
        if not pd.api.types.is_numeric_dtype(data_df['ratio']):
            consistency_issues += 1
        
        metrics['consistency'] = (1 - consistency_issues / 2) * 100
        
        # ì¶©ë¶„ì„± (í‚¤ì›Œë“œë³„ ìµœì†Œ ë°ì´í„° ê°œìˆ˜)
        keyword_counts = data_df['keyword'].value_counts()
        min_required = 12
        sufficient_keywords = (keyword_counts >= min_required).sum()
        metrics['sufficiency'] = (sufficient_keywords / len(keyword_counts)) * 100
        
        # ì‹ ì„ ë„ (ìµœì‹  ë°ì´í„° ë¹„ìœ¨)
        if 'date' in data_df.columns and pd.api.types.is_datetime64_any_dtype(data_df['date']):
            latest_date = data_df['date'].max()
            six_months_ago = latest_date - timedelta(days=180)
            recent_data_count = (data_df['date'] >= six_months_ago).sum()
            metrics['freshness'] = (recent_data_count / len(data_df)) * 100
        else:
            metrics['freshness'] = 0
        
        # ê· í˜•ì„± (í‚¤ì›Œë“œë³„ ë°ì´í„° ë¶„í¬ì˜ ê· ë“±ì„±)
        keyword_counts = data_df['keyword'].value_counts()
        if len(keyword_counts) > 1:
            cv = keyword_counts.std() / keyword_counts.mean()  # ë³€ë™ê³„ìˆ˜
            metrics['balance'] = max(0, (1 - cv) * 100)
        else:
            metrics['balance'] = 100
        
        return metrics
    
    def _calculate_outlier_metrics(self, outlier_results: Dict) -> Dict[str, float]:
        """ì´ìƒì¹˜ ê´€ë ¨ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
        if not outlier_results or 'summary' not in outlier_results:
            return {'outlier_percentage': 0.0}
        
        return {
            'outlier_percentage': outlier_results['summary']['outlier_percentage'],
            'outlier_severity': min(100, outlier_results['summary']['outlier_percentage'] * 2)
        }
    
    def _calculate_prediction_metrics(self, validator_results: Dict) -> Dict[str, float]:
        """ì˜ˆì¸¡ ê´€ë ¨ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
        if not hasattr(self, 'validation_results') or not validator_results:
            return {}
        
        # ì˜ˆì¸¡ê°’ ë³´ì • ë¹„ìœ¨ ê³„ì‚°
        total_predictions = len(validator_results)
        corrected_predictions = sum(1 for r in validator_results.values() if r.get('corrections_made', False))
        
        prediction_reliability = (1 - corrected_predictions / total_predictions) * 100 if total_predictions > 0 else 100
        
        return {
            'prediction_reliability': prediction_reliability,
            'prediction_corrections': corrected_predictions
        }
    
    def _calculate_overall_score(self, metrics: Dict[str, float]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            'completeness': 0.2,
            'validity': 0.25,
            'consistency': 0.15,
            'sufficiency': 0.15,
            'freshness': 0.1,
            'balance': 0.1,
            'outlier_percentage': -0.05  # ìŒì˜ ê°€ì¤‘ì¹˜ (ì´ìƒì¹˜ê°€ ë§ì„ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ)
        }
        
        score = 0
        total_weight = 0
        
        for metric, weight in weights.items():
            if metric in metrics:
                if metric == 'outlier_percentage':
                    # ì´ìƒì¹˜ëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (5% ì´í•˜ë¥¼ 100ì ìœ¼ë¡œ ì„¤ì •)
                    normalized_value = max(0, 100 - metrics[metric] * 20)
                else:
                    normalized_value = metrics[metric]
                
                score += normalized_value * abs(weight)
                total_weight += abs(weight)
        
        return score / total_weight if total_weight > 0 else 0
    
    def _determine_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë“±ê¸‰ì„ ê²°ì •í•©ë‹ˆë‹¤"""
        if score >= 90:
            return "A (ìš°ìˆ˜)"
        elif score >= 80:
            return "B (ì–‘í˜¸)"
        elif score >= 70:
            return "C (ë³´í†µ)"
        elif score >= 60:
            return "D (ë¯¸í¡)"
        else:
            return "F (ë¶ˆëŸ‰)"
    
    def _check_thresholds(self, metrics: Dict[str, float]) -> Dict[str, bool]:
        """ì„ê³„ê°’ í†µê³¼ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤"""
        passed = {}
        
        threshold_mapping = {
            'completeness': 'completeness_min',
            'validity': 'validity_min',
            'consistency': 'consistency_min',
            'sufficiency': 'sufficiency_min'
        }
        
        for metric, threshold_key in threshold_mapping.items():
            if metric in metrics:
                if threshold_key in self.quality_thresholds:
                    passed[metric] = metrics[metric] >= self.quality_thresholds[threshold_key]
                else:
                    # ì„ê³„ê°’ì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                    default_thresholds = {
                        'completeness_min': 85.0,
                        'validity_min': 90.0,
                        'consistency_min': 95.0,
                        'sufficiency_min': 70.0
                    }
                    default_value = default_thresholds.get(threshold_key, 0)
                    passed[metric] = metrics[metric] >= default_value
        
        # ì´ìƒì¹˜ëŠ” ë°˜ëŒ€ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        if 'outlier_percentage' in metrics:
            outlier_threshold = self.quality_thresholds.get('outlier_max', 5.0)
            passed['outlier_percentage'] = metrics['outlier_percentage'] <= outlier_threshold
        
        return passed
    
    def _generate_alerts(self, metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ê²½ê³ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        alerts = []
        current_time = datetime.now()
        
        # ì„ê³„ê°’ ê¸°ë°˜ ê²½ê³ 
        completeness_threshold = self.quality_thresholds.get('completeness_min', 85.0)
        if metrics.get('completeness', 100) < completeness_threshold:
            alerts.append({
                'type': 'warning',
                'category': 'completeness',
                'message': f"ë°ì´í„° ì™„ì„±ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({metrics['completeness']:.1f}% < {completeness_threshold}%)",
                'severity': 'medium',
                'timestamp': current_time
            })
        
        validity_threshold = self.quality_thresholds.get('validity_min', 90.0)
        if metrics.get('validity', 100) < validity_threshold:
            alerts.append({
                'type': 'error',
                'category': 'validity',
                'message': f"ë°ì´í„° ìœ íš¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ ({metrics['validity']:.1f}% < {validity_threshold}%)",
                'severity': 'high',
                'timestamp': current_time
            })
        
        outlier_threshold = self.quality_thresholds.get('outlier_max', 5.0)
        if metrics.get('outlier_percentage', 0) > outlier_threshold:
            alerts.append({
                'type': 'warning',
                'category': 'outliers',
                'message': f"ì´ìƒì¹˜ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤ ({metrics['outlier_percentage']:.1f}% > {outlier_threshold}%)",
                'severity': 'medium',
                'timestamp': current_time
            })
        
        sufficiency_threshold = self.quality_thresholds.get('sufficiency_min', 70.0)
        if metrics.get('sufficiency', 100) < sufficiency_threshold:
            alerts.append({
                'type': 'info',
                'category': 'sufficiency',
                'message': f"ë°ì´í„° ì¶©ë¶„ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({metrics['sufficiency']:.1f}% < {sufficiency_threshold}%)",
                'severity': 'low',
                'timestamp': current_time
            })
        
        return alerts
    
    def _generate_recommendations(self, metrics: Dict[str, float]) -> List[str]:
        """í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤"""
        recommendations = []
        
        if metrics.get('completeness', 100) < 90:
            recommendations.append("â€¢ ê²°ì¸¡ê°’ ì²˜ë¦¬ ë¡œì§ì„ ê°œì„ í•˜ì„¸ìš”")
            recommendations.append("â€¢ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œ ëˆ„ë½ì„ ë°©ì§€í•˜ì„¸ìš”")
        
        if metrics.get('validity', 100) < 95:
            recommendations.append("â€¢ ì…ë ¥ ë°ì´í„°ì˜ ë²”ìœ„ ê²€ì¦ì„ ê°•í™”í•˜ì„¸ìš”")
            recommendations.append("â€¢ ë°ì´í„° ë³€í™˜ ê³¼ì •ì—ì„œì˜ ì˜¤ë¥˜ë¥¼ ì ê²€í•˜ì„¸ìš”")
        
        if metrics.get('outlier_percentage', 0) > 3:
            recommendations.append("â€¢ ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ ê·œì¹™ì„ ì¬ê²€í† í•˜ì„¸ìš”")
            recommendations.append("â€¢ ë°ì´í„° ìˆ˜ì§‘ ì†ŒìŠ¤ì˜ ì‹ ë¢°ì„±ì„ í™•ì¸í•˜ì„¸ìš”")
        
        if metrics.get('sufficiency', 100) < 80:
            recommendations.append("â€¢ ê° í‚¤ì›Œë“œë³„ë¡œ ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”")
            recommendations.append("â€¢ ë¶„ì„ ê¸°ê°„ì„ ëŠ˜ë ¤ ì¶©ë¶„í•œ ìƒ˜í”Œì„ í™•ë³´í•˜ì„¸ìš”")
        
        if metrics.get('balance', 100) < 70:
            recommendations.append("â€¢ í‚¤ì›Œë“œë³„ ë°ì´í„° ìˆ˜ì§‘ëŸ‰ì˜ ê· í˜•ì„ ë§ì¶”ì„¸ìš”")
            recommendations.append("â€¢ í¸í–¥ëœ í‚¤ì›Œë“œì— ëŒ€í•œ ì¶”ê°€ ìˆ˜ì§‘ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        if metrics.get('freshness', 100) < 60:
            recommendations.append("â€¢ ë” ìµœì‹ ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”")
            recommendations.append("â€¢ ì •ê¸°ì ì¸ ë°ì´í„° ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ì„ ì„¤ì •í•˜ì„¸ìš”")
        
        return recommendations
    
    def _log_assessment(self, assessment_result: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ë¥¼ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤"""
        log_entry = {
            'timestamp': assessment_result['timestamp'].isoformat(),
            'overall_score': assessment_result['overall_score'],
            'quality_grade': assessment_result['quality_grade'],
            'alert_count': len(assessment_result['alerts']),
            'data_count': assessment_result['data_count']
        }
        
        self.monitor_log.append(log_entry)
        
        # ë¡œê·¸ íŒŒì¼ì—ë„ ì €ì¥
        log_file = os.path.join(Config.SAVE_DIR, 'quality_monitor.log')
        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_quality_dashboard(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        if not self.quality_history:
            return {"message": "í’ˆì§ˆ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        latest_assessment = self.quality_history[-1]
        
        # ìµœê·¼ ê²½ê³ ë“¤ (24ì‹œê°„ ì´ë‚´)
        recent_alerts = [
            alert for alert in self.alerts 
            if alert['timestamp'] > datetime.now() - timedelta(hours=24)
        ]
        
        # í’ˆì§ˆ íŠ¸ë Œë“œ (ìµœê·¼ 5ê°œ í‰ê°€)
        recent_assessments = self.quality_history[-5:]
        quality_trend = [a['overall_score'] for a in recent_assessments]
        
        # ê²½ê³  í†µê³„
        alert_stats = {
            'total': len(recent_alerts),
            'high': len([a for a in recent_alerts if a['severity'] == 'high']),
            'medium': len([a for a in recent_alerts if a['severity'] == 'medium']),
            'low': len([a for a in recent_alerts if a['severity'] == 'low'])
        }
        
        dashboard = {
            'last_updated': latest_assessment['timestamp'],
            'current_score': latest_assessment['overall_score'],
            'current_grade': latest_assessment['quality_grade'],
            'data_summary': {
                'total_records': latest_assessment['data_count'],
                'keywords': latest_assessment['keyword_count']
            },
            'quality_metrics': latest_assessment['quality_metrics'],
            'quality_trend': quality_trend,
            'recent_alerts': recent_alerts,
            'alert_statistics': alert_stats,
            'threshold_status': latest_assessment['passed_thresholds'],
            'recommendations': latest_assessment['recommendations']
        }
        
        return dashboard
    
    def export_quality_report(self, filename: str = None) -> str:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"quality_report_{timestamp}.json"
        
        report_data = {
            'generated_at': datetime.now().isoformat(),
            'quality_thresholds': self.quality_thresholds,
            'assessment_history': [
                {
                    **assessment,
                    'timestamp': assessment['timestamp'].isoformat()
                }
                for assessment in self.quality_history
            ],
            'dashboard': self.get_quality_dashboard()
        }
        
        # timestampë¥¼ ë‹¤ì‹œ isoformatìœ¼ë¡œ ë³€í™˜
        if 'dashboard' in report_data and 'last_updated' in report_data['dashboard']:
            report_data['dashboard']['last_updated'] = report_data['dashboard']['last_updated'].isoformat()
        
        for alert in report_data['dashboard'].get('recent_alerts', []):
            alert['timestamp'] = alert['timestamp'].isoformat()
        
        filepath = os.path.join(Config.SAVE_DIR, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            print(f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥: {filepath}")
            return filepath
        except Exception as e:
            print(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def print_quality_summary(self, assessment_result: Dict[str, Any] = None):
        """í’ˆì§ˆ í‰ê°€ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤"""
        if not assessment_result and self.quality_history:
            assessment_result = self.quality_history[-1]
        
        if not assessment_result:
            print("í’ˆì§ˆ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ í‰ê°€ ìš”ì•½")
        print("="*60)
        
        print(f"í‰ê°€ ì‹œê°„: {assessment_result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ë°ì´í„° ê·œëª¨: {assessment_result['data_count']:,}ê°œ ë ˆì½”ë“œ, {assessment_result['keyword_count']}ê°œ í‚¤ì›Œë“œ")
        print(f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {assessment_result['overall_score']:.1f}ì ")
        print(f"í’ˆì§ˆ ë“±ê¸‰: {assessment_result['quality_grade']}")
        
        print(f"\nğŸ“ˆ í’ˆì§ˆ ë©”íŠ¸ë¦­:")
        metrics = assessment_result['quality_metrics']
        for metric, value in metrics.items():
            status = "âœ…" if assessment_result['passed_thresholds'].get(metric, True) else "âŒ"
            print(f"  {status} {metric}: {value:.1f}%")
        
        if assessment_result['alerts']:
            print(f"\nâš ï¸  ê²½ê³  ì‚¬í•­ ({len(assessment_result['alerts'])}ê°œ):")
            for alert in assessment_result['alerts']:
                severity_icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸ”µ"}.get(alert['severity'], "â„¹ï¸")
                print(f"  {severity_icon} {alert['message']}")
        
        if assessment_result['recommendations']:
            print(f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            for rec in assessment_result['recommendations']:
                print(f"  {rec}")
        
        print("="*60)
    
    def clear_history(self, days_to_keep: int = 30):
        """ì˜¤ë˜ëœ ì´ë ¥ì„ ì •ë¦¬í•©ë‹ˆë‹¤"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        # í’ˆì§ˆ ì´ë ¥ ì •ë¦¬
        self.quality_history = [
            assessment for assessment in self.quality_history
            if assessment['timestamp'] > cutoff_date
        ]
        
        # ê²½ê³  ì´ë ¥ ì •ë¦¬
        self.alerts = [
            alert for alert in self.alerts
            if alert['timestamp'] > cutoff_date
        ]
        
        print(f"âœ… {days_to_keep}ì¼ ì´ì „ì˜ í’ˆì§ˆ ì´ë ¥ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.") 